

Section 1: Kubernetes Overview


	1.	Containers Overview
		
		-	Kubernetes is developed by Google based on their experience with running containers on the prod env.
		
		a.	Docker Containers
			
			- 	Requirement to develop the end to end stack with various technologies
				-	Ex: Mongo DB, Redis, Nodejs, orchestration like Ansible 				
				-	Version of services mismatch with OS
				-	Compatibility b/w services and libraries, dependencies on the OS
				-	One service requires one version of dependencies and another requires another one
				-	Long time for setup of local env
				-	Different Dev/Test/Prod env
		
			-	What are Containers
				-	Container are completely isolated env and have thier own service and processes, mounts, networking interfaces
				-	
			- 	Linux Based OS 
				-	An OS has two diff components
				-	Kernel and Set of s/w libraries
				-	All Linux based share the same kernel and s/w on top of kernel makes them diff
				-	S/w makes the OS diff
			
			- Sharing the Kernel
				-	If the underlying is Ubuntu then docker can run container of any flavor of Linux based OS
				-	Each docker container will have additional s/w that make the OS different
				-	Docker uses the underlying OS kernel and which works with all the os
				-	Docker is not meant to virtualize and run diff os, kernels on same h/w
				-	Main purpose of dockers are to containerize the app and run them and ship them
				-	

Section 2: Setting Up env

	1.	Download the Virtual Box
	2.	Download the os from os box https://www.osboxes.org
	3.	Extract the 7zip and there will be and vdi image
	4.	We can use that vdi image to create virtual server
	5.	Enable to the virtualization from the BIOS setup
	6.	Changing the n/which
		-	Change the n/w to bridge from NAT
		-	This will bridge n/w between network interface of VM and wireless n/w adapter of my laptop
	7. 	Install the mobaXterm that is used to ssh to servers
	8.	Configuring the SSH Service on the Linux host
		-	service ssh status
		-	Install the SSH service	package
			-	apt-get update
			-	apt-get install openssh-server
	9. Setting Up Docker in Linux
		-	apt-get install docker.io
		-	
		
Section 3: Container Orchestration
	
	1.	Orchestration means managing the containers
	2.	Container Orchestration is a process of deploying and managing the containers automatically.
		-  Increasing and removing nodes automatically based on the load
	3.	Container Orchestration Technologies currently available on the market
		-	Docker Swarm
			-	Easy to configure but does have advanced feature for complex applications
		-	kubernetes
			-	Bit difficult to setup 
			-	Provides lot of options for customized deployments
			-	Is supported from all the Cloud service providers like AWS, Google Cloud, Azure 
			-	Kubernetes project is one of the top ranked project on the Git Hub
		-	Mesos
			- 	Difficult to configure and get started
			-	Supports all the advanced options 
	
	4. Advantages of Container Orchestration
		
		-	Application will be highly available
		-	Resistance to Hardware Failures as there will multiple instances of application running on diff nodes
		-	User Traffic will be load balanced across the various containers
		-	When demand increases, deploy more instances of application seamlessly in a matter of seconds
		-	And we have a ability to do that at service level when we run out of hardware resources
		-	Scale the number of nodes up or down without having to take down the application
		-	We can do all this with a set of object configuration files
		-	That is Kubernetes
			- It is a container orchestration technology that is used to orchestrate the deployment and management of hundreds of nodes
		
	
	
	
Section 4: Kubernetes Architecture

	1.	Node - A node is a machine, a physical or virtual on which Kubernetes is installed and runs
		-	A node is a worker machine  and that is where container will be launched by kubernetes
		-	It is also known as minions in the past
	2.	Node Failure - To overcome single node failure we will have cluster of nodes
		-	Having multiple nodes also helps in sharing the loads across the clusters
	3.	Master Node -	is a another node with kubernetes installed on it and configured as Master 
		- Master node watches over the nodes on the cluster and is actually responsible for orchestration
	4.	When we installing the kubernetes we will install all the below components
	
	
	5.	Kubernetes components
		
		a.	API Server
		b.	etcd
		c.	Kublet
		d.	Container Runtime
		e.	Controller
		f.	Scheduler
		
		a.	API Server
			
			- 	Acts as front-end for kubernetes
			-	Users, Management devices, Command-line interfaces all talks to API Server to interact with Kubernetes cluster
		
		b. 	Etcd Keystore
			
			-	Etcd is distributed reliable key value store used by kubernetes
			-	etcd is used to store all the information to manage the cluster
			-	In a multi node and multi manager cluster architecture etcd stores all the information on all the nodes in the cluster
			- 	etcd is responsible for maintaining locks with in cluster to ensure that there will be no conflict b/w the clusters
			
		c.	Scheduler 
			
			-	Is responsible for  distributing the work or containers across the nodes
			-	It looks for newly created containers and assigns them to node
		
		d.	Controllers
			
			-	Controllers are brain behind orchestration
			-	They are responsible for noticing  and responding when nodes, containers and endpoints went down
				-	Controllers makes decision of bringing up new containers in such cases
		e.	Container Runtime
			
			-	Is the underlying s/w that is used to run containers
			-	Ex : docker , other Rocket, crio
		
		f. 	Kubelet
		
			-	Is a agent that runs on each node of the cluster
			-	Agent is responsible for making sure that containers are running as excepted on the node
		
		
	6. Components in Master and Worker Node
		
		-	Worker Node
			-	Container Runtime to run Docker container on the system
			- Kublet - agent
		- 	Master Node
			-	Kube API server
			-	Etcd
			-	controller 
			-	scheduler
			
		
	7.	Kubectl
	
		-	Kube Command Line Utility or Kube Control
			-	KubeCtl is used to deploy and manage applications on a cluster
			- 	To get status of nodes in a cluster and to manage other things
		
			 	kubectl run hello-minikube
				
					-	Is used to deploy an application on the cluster
					
				kubectl cluster info 
					
					- 	Is used to view information about cluster
				
				kubeclt get nodes 
					-  Is used to list all the nodes part of cluster
					



Section 5: Kubernetes Setup and Minikube


	1.	Ways to setup kubernetes
		
		a. 	Local
			
			-	we can setup ourself locally on our laptops or VMs using solutions like minikube and kubeadmins
			-	Minikube is a tool used to setup a single instances of kubernetes in all in one setup
			-	Kubeadmin is a tool used to configure kubernetes in a multi-node setup on our local machines
		
		b.	Hosted Solutions
			
			-	There are hosted solutions available for setting kubernetes on the cloud env such as GCP, AWS

		c.	play-with-k8s.com	
					
		
	2.	Minikube
		
		a.	Minikube bundles all the different components into a single image providing pre-configured single node kubernetes cluster
		b.	whole bundle is available as an iso image and available on the online
		c.	Minikube provides an executable command line utility that automatically downloads ISO and deployed in a virtualization platform such as VM ware or
			virtual box
		d.	so we must installed a hypervisor s/w like Virtual Box or HyperV
		e.	And finally we should have a kubernetes command line tool(kubectl) installed on your machine
		
		
		
		
Section 6: Kubernetes Setup and Demo Minikube

	1.	Demo Minikube
		
		a.	Enable Hypervisor from BIOS
		b.	Install a Hypervisor
			-	Virtual Box, VMware, Hyper-V
		c.	Install Kubectl 
			- 	Download kubectl.exe
			-	Save in C folder
		d. 	Install minikube
			-	rename with .exe
			- 	Save in same folder of kubectl.exe
		e.	Execute the minikube installer
		
			-	cd C:/kubernetes
			-	minikube.exe start
				-	this will install new vm minikube on the virtual box
				-	also install single node kubernetes cluster
		
		f.	Make sure kubernetes is working fine
		
			-	kubectl get nodes
			-	kubectl get pods
			-	kubectl run	hello-minikube --image=k8s.gcr.io/echoserver:1.4 --port=8080
			-	kubectl get pods 
			- 	kubectl expose  deployment hello-minikube --type=Nodeport
			-	minikube service hello-minikube --url
			-	kubectl delete  deployment hello-minikube
	
			
Section 7: Kubeadm Setup

	1. 	kubeadm helps to setup multinode cluster with master and worker nodes on a separate machines
	2.	Kubeadm also makes container management very easy
	3. 	Steps to set up kubeadm
		1.	We must have multiple systems or VMs created for configuring a cluster
			-	Once systems are ready designate one system as master and other as worker node
		2.	Install Docker on all the nodes
		3.	Install  kubeadm on all the nodes
			-	kubeadm will bootstrap the kubernetes solutions	by installing and configuring right components on the right nodes
		4.	Initialize the master server 
			- 	During this process all the required components are installed and configured on the master server
		5.	Create a POD N/w for communication b/w master and worker node
		6.	Join the worker node to master node

		
Section 8: Kubeadm Demo	Part -1

	1.	Follow the steps in 	https://kubernetes.io/docs/setup/independent/install-kubeadm/
	2.	Create 3 VMs with one of the network adapter as Bridge
	3.	Create a cloned copy of 2 other VMs by reinitializing MAC address with type of Linked clone
		-	Full Clone 
			-	Exact copy
			- 	Takes more space
		-	Linked Clone
			-	Takes less space
	4.	Enable SSH on all the VMs
	5.	Change the hostname and host file on all the 3 VMs
		-	vi /etc/hostname
			kubemaster
			
		- 	vi /etc/hosts
				127.0.0.1	localhost
				127.0.1.1	kubemaster
	
		- Restrat the VMs
	
	6.	Currently we have only Bridge N/w and create a new Host Only N/w
		-	These IPs are assigned by Wifi router
		-	These IPs provided DHCP service on the router
		-	IP from DHCP are dynamic and they are not guaranteed to same IP address everytime
		-	They will change when VMs got restart
		- 	This is not going to work properly with kubernetes
		-	Because communication b/w master and worker node happens through internal n/w and rely on these IP address to communicate with each other
		-	So these hosts must static IP address assigned to them
		-	We can't create static IP address with Bridge N/w
		-	So an alternate way is to create a new network which is Host Only Network
		-	That N/w will be used as Kubernetes N/w for interaction or communication b/w nodes
		-	When we create a Host Only N/w then we are actually disabling the DHCP
		-	Host Only N/w will be created inside the my laptop
		-	Laptop will have a Virtual Interface
		-	All VMs will Virtual Interface on the N/w and we can assign static IP address on that n/w
		-	Create or use Host Only N/w already created in Global tools
		-	Create a new n/w adapter with Host-only n/w
	
	7.	Assign the IP address to new n/w interface
		-	Temporary assignment 
			
				ifconfig	enp0s8	192.168.56.2
		
		-	Permanent	assignment of IP address
				
				vi /etc/network/interfaces
					
					#Configure enp0s8 interface
					auto enp0s8
					iface enp0s8 inet static
						address	192.168.56.2
						netmask 255.255.255.0
		
		- reboot the system
	
	8. Disable the Swapoff
		
		-	swapoff -a
		-	vi /etc/fstab
		
			-	comment the last line having dump as swap
	
	
	
	
	
Section 9: Kubeadm Demo	Part -2	
	
	1.	Install docker on all the nodes
		
		a. 	Doc or instruction the kubernetes will install latest docker this may result some incompatibilty
		b.	Follow the instruction Docker website for ubuntu
		c.	Dont install the latest verison
		d.	Install docker that is compatible by checking on the kubernetes project of Github
	
	2.	Install Kubeadm
		
		a.	Follow the instructions on the kubeadm page 
		b. 	Run the command listed in page
		c. 	Install the kubeadm, kubelet, kubeclt on all the nodes
	
	3. Initialize Kubemaster on master node
		
		a.	Initialize kubeadm on master with two agruments
			
				kubeadm init <args> 
			
			- 	Pod network cidr
				
				-	10.244.0.0/16
			
			-	apiserver-advertise-address=192.168.56.2
				
				- 	This should be static addresss
				-	This is master configures the API server to listen to commands
				
	4.	Join the worker nodes to master node
		
		a.	Join the worker nodes with command 
		
			kubeadm join --token <token> <master-ip>:<master-port> --discovery-token-ca-cert-hash sha256:<hash>
				
				
		b.	run containers
			
			kubclt run nginx --image=nginx
			
			kubectl get nodes
			
			kubectl get pods
			
			
			
	
Section 10: POD
		
	1.	POD is an small object that we can create on the Kubernetes
	2.	To bring new instance of an application then we will new POD with instance of an application
	3.	To handle more traffic we need to create a new instance of application with in POD with in same node or with new node
	4.	POD will have one-to-one relationship with the container
	5.	A POD can have more than one container but each with different type
	6.	Ideally we will not use multicontainer POD
	7.	POD will link all the containers within a POD, so no need to configure it manually
	8.	Creating a container with POD	
	
			kubectl run nginx
			
			- 	It first creates a POD and deploys an instance of nginx container within the POD
			
	9.	We also need to specify the image name
	
			kubectl run nginx --image nginx
			
			kubectl get pods
			
	
	
Section 10: POD - Demo

	1. 	We need to execute container run command on the kube master
	2.	Kube master will create a PODs on the worker nodes
	3.	Command to see the POD details 
	
			kubeclt run nginx --image=nginx
			
			kubectl describe pods
			
			kubectl	 get pods -o wide
			
	
		
	
Section 11: POD with YML	

	1.	YML in Kubernetes
		
		a.	Root Elements on pod-definition.yml file
		
				apiVersion: v1
				kind: Pod
				metadata:
					name: myapp-pods
					labels:
						app: myapp
						type: front-end
				spec:
					contiainers:
						- name: nginx-contianer
						  image: nginx
				
		b.	Values of apiVersion and kind

				kind		Version
				
				POD			v1
				Service		v1
				ReplicaSet	apps/v1
				Deployment	apps/v1
				
		c.	Once poc-definition.yml is created run the command
		
		
			kubctl create -f pod-definition.yml
			
			kubectl get pods
			
			kubectl describe pod myapp-pod


	2. 	Tricks with YML file

		a.	Kubernets and openshit plugin is available of webstrom, IntelliJIdea
		b.	With this plugin we can develop kubernetes specfic autocompletion on the IDEs
	
	

	
Section 12: Replication Controller

	1.	Kubernets Controller 
	
		-	Controllers are the brain behind the kubernetes 
		-	They are processes  that monitors kubernetes objects and responds accordingly
	
	
	2. Replication Controller
		
		a.	Replication controller helps to run multiple instances of POD in a kubernetes cluster providing high availability
		b.	Replication controller works in single pod, even if the single pod fails Replication Controller brings the new pod
		c.	Replication Controller ensure that specified number of nodes are running at all the time
		d.	Replication controller also helps in Load Balancing and Scaling 
			-	RC automatically brings the additional pods on same node or on the cluster
			- 	RC automatically balances the load across the PODS of nodes in a cluster
		e.	Replica Set is a newer version of Replication Controller
		
	
	3.	RC Definition file
		
		a. Ex configuration file
			
			apiVersion: v1
			kind: ReplicationController
			metadata:
				name: myapp-rc
				lables:
					app: myapp
					type: front-end
			
			spec:
				template:
					metadata:
						name: myapp-pods
						labels:
							app: myapp
							type: front-end
					spec:
						containers:
							- name: nginx-contianer
							  image: nginx
			
		
				replicas: 3
				
			
		b. commands to ReplicationController
		
				kubectl create -f rc-definition.yml
				
				kubectl get replicationcontroller
				
				kubectl get pods
				
		
	4. 	Replica Set
	
		a.	Ex of configuration file
		
		
			apiVersion: apps/v1
			kind: ReplicaSet
			metadata:
				name: myapp-replicaset
				lables:
					app: myapp
					type: front-end
			
			spec:
				template:
					metadata:
						name: myapp-pod
						labels:
							app: myapp
							type: front-end
					spec:
						containers:
							- name: nginx-contianer
							  image: nginx
			
		
				replicas: 3
				selector:
					matchLabels:
						type: front-end
						
			
		b. selector parameter in the spec dictionary is the major difference b/w ReplicaSet and ReplicationController
		
		
		c.	Commands to Create Replica Set
		
				kubectl create -f replicaset-definition.yml
				
				
				kubectl get replicaset
				
				kubectl get pods
				
	5. 	Labels and Selectors
	
		a. Scenario
		
			- 	Say we deployed three instances of our frontend application as three pods.
			- 	We would like to create a replication controller or replica set to ensure that we have three active pods at any time
			-	This one of the use case of replica sets we can use it to monitor existing pods if we have them already created
			-	In-case its not created previously, the replica set will create for us
			-	The replica set is in fact a process that monitors the pods.



		b.	There could be hundreds of other parts in the cluster running different applications.
			
			-	This is where labelling our pods during creation comes in handy.
			-	We could now provide these labels as a filter for replica set 
			-	Under the selector section we use to match labels filter
			-	Provide the same label that we used while creating the pods.

				This way the replica set knows which part to monitor.
				
		c.	Commands
				
				kubectl create -f replicaset-definition.yml
				
				kubectl get replicaset
				
				kubectl delete  replicaset myapp-replicaset
				
					-	Also deletes the underlying PODS
				
				kubectl	replace -f replicaset-definition.yml
				
				kubectl scale -replicas=6 -f replicaset-definition.yml
				
				
		
Section 13: Deployments


	1. 	What is kubernetes Deployments
	
		1.	A Web server that needs to be deployed in a production environment.
		2.	We need multiple instances of Web server
		3.	When ever new version of application build become available on the docker registry we need to upgrade docker instances seamlessly
		4.	This may impact users accessing our applications so you might want to upgrade them one after the other
			-	And that kind of upgrade is known as rolling updates
		5.	Suppose one of the upgrades performed resulted in an unexpected error and we're asked to undo the recent change
			-	We would like to be able to roll back the changes that were recently carried out
		6.	Finally say for example we would like to make multiple changes to your environment such as upgrading
			-	The underlying Web Server versions as well as scaling your environment and also modifying the resource allocations etc
			-	We do not want to apply each change immediately after the command is run, instead you like to apply a pause to your environment
			- 	Make the changes and then resumes so that all the changes are rolled out together
		7.	All of these capabilities are available with the Kubernetes deployments. 		
		
	2.	Kubernetes Deployment 
	
		-	PODS deploys a single instance of the application 
		-	Each container is encapsulated with POD
		-	Multiple such pods are deployed using replication controllers or replica sets
		-	Deployment is a kubernetes Object that comes in higher the hierarchy 
		-	The deployment provides us with the capability 
			-	To upgrade the underlying instances seamlessly using rolling updates
			-	Undo changes and pause and resume changes as required

	3.	How to create Deployment definition file
		
		-	Deployment definition file is exactly same like replica set 
		- 	Kind wiht Deployment
		
	4. Commands to create Deployment	
	
		-	kubeclt create -f deployment-definition.yml
		-	kubectl get deployments
		-	kubectl get replicaset
		-	kubectl get pods
		- 	kubectl get all
		- 	kubectl describe deployment
		
	


	
	
Section 13: Rollout and verisoning

	1.	When we first create deployment it creates a rollout and it triggers a deployment revision
	2.	When we make changes to containers or upgrade it will create a new rollout and new revision
	3.	When something is not correct in new deployment we can easily earlier revision
	4.	Rollout Commands

			kubectl rollout status deployment/myapp-deployment
			kubectl rollout history deploy/myapp-deployment
			
			
	1.	Deployment Strategy

		a. 	Recreate
		
			- 	All the current instances will be brought down and new instances will be launched
			-	Application will be down for few time
		
		b.	Rolling
			
			-	All the instances aren't brought down at same time
			-	Changes will applied one by one 
			-	No down time
			-	Rolling Deployment is the default Deployment Strategy
			
	2.	kubectl apply
	
		- 	Apply command is used to apply the changes to deployment that are made changes in Deployment file
		
			kubectl apply  -f deployment-deployment.yml
		
		-	updating the deployment at runtime
		
			kubectl set image deployment/my-app-deployment \
				nginx=nginx:1.9.1
				
	3. Deployment upgrades
	
		a.	How deployment applies the changes
		
			-	It first creates replica set that needs to deployed
			-	It then starts replacing the old pods with new one
		
		
				kubectl get replicasets
		
		b.	Rollingback to previous revisions
		
			-	We can rollback the changes using below command
			
				kubectl  rollout undo  deployment/myapp-deployment
				
				kubectl get replicasets
				
		c.	kudectl run command 
		
			- 	run command also creates the deployment along with the pod
			-	This is one of the creating deployment without the definition file
			- 	Run command automatically creates the definition file and replicaset
				
				kubectl run nginx --image=nginx-contianer
				
		f.	Commands
		
			kubectl create -f deployment-definition.yml
			kubectl get deployments
			kubectl update -f deployment-definition.yml
			kubeclt set image deployment/myapp-deployment nginx=nginx:1.9.1
			kubectl rollout status deployment/myapp-deployment 
			kubectl rollout history deployment/myapp-deployment
			kubectl rollout  undo deployment/myapp-deployment
	
	
	
	
	
Section 14: Networking in kubernetes

	1.	In Docker world IP address is assigned to Docker Container
	2.	But in Kubernetes IP address is assigned to POD
	3.	Each POD in kubernetes gets its own IP address
	4.	When kubernetes is initially configured then  create and internal private n/w with address 10.244.0.0
	5.	When we deploy multiple Pods they get an IPAddress assigned form this n/w
	6.	Pods can communicate with each other using these IPAddress but its not a good idea as IPAddress keeps on changing when Pods are created
	7.	Kubernetes expects us to setup the n/w solutions
	8.	There are wide variety of solutions already available 
	9.	kubeclt get pods -n attom -o wide
	


Section 15:	Kubernetes Services

	1.	Service enables communication b/w various components with in and outside of the application
	2.	Services helps to connect applications together with other applications or users
	3.	Application may have various group of pods running various section such as group for running backend processes and one group running frontend
	4.	Its helps communication b/w  back end and front end pods and helps in establishing connectivity to external n/w
	5.	Services enables loose coupling b/w microservices in our application
	
	
	How to access the web server running on the pod on the kubernetes node

		- 	Service listens to a port on the node and forwards request to a port on the pod
		- 	This type of service is known as node port service
	
	Services Types
	
		1. 	NodePort
		
			- 	Service listens to a port on the node and forwards request to a port on the pod
			-	There are 3 ports involved in Node Port service
				1.	Target Port	
					- 	Port on the pod where webserver is running
				
				2. 	Service Port
					- 	Port on the service itself
					- 	Service is like a virtual service inside the node
					-	Inside the cluster it has its own IPAddress and its known as Cluster IPAddress
				
					-	
				3.	NodePort
					-	The port on the node itself which we can use to access the webserver 
					-	Node port can be in the range 30000 to 32767

		2.	Cluster IP
			
			-	Service creates a virtual IP inside a cluster to enable communication b/w different services such as front end servers and back end servers
			
		3.	Load Balancer
			
			-	Load balancer provisions a load balancer to the application that distributes load across the nodes 
	
	
	
	How to create a service 
	
		 -  Just like a deployments, pods and replica sets we need to create a service-definition.yml file with all the specification 
		 -	Service-definition.yml
				
				apiVerision: v1
				kind: Service
				metadata:
					name:  myapp-service
				spec:	
					type: NodePort
					ports:	
						- targetPort: 80
						  port: 80
						  nodePort: 30008
					selector:
						app: myapp
						type: front-end
						
			
			
			kubectl create -f service-definition.yml
			kubeclt get services
			
	
	What do we do when we have mupltiple pods in single node

		-	We have multiple instances of web application running for higher availability and load Balancing purposes
		-	So when the service is created it looks for matching pod with label and finds three of them
		-	The service then automatically selects all the three pods as endpoints and forwards to all the pods
		-	Service use the random algorithm to distribute the load across the nodes
		
		
		
	What do we do when we have multiple pods on different nodes

		-	kubernetes will create a service that span across the nodes in the cluster
		-	Maps the TargetPort to same NodePort on all the nodes in cluster
		-	This way we can access our application using IP of any node in the cluster using the same node port number 30008

		
		
	
		
	

	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
				
				
		
	
	
	
	
	
	
	
	
	
		